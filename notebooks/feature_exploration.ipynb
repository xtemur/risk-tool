{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering Analysis\n",
    "Complete analysis of feature engineering pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "sys.path.append(str(Path('..').resolve()))\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "from src.data.database_manager import DatabaseManager\n",
    "from src.features import TechnicalFeatures, BehavioralFeatures, MarketRegimeFeatures, FeaturePipeline\n",
    "\n",
    "# Setup\n",
    "pd.set_option('display.max_columns', 30)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "\n",
    "db = DatabaseManager(\"../data/trading_risk.db\")\n",
    "print(\"Database initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all accounts\n",
    "accounts = db.get_accounts()\n",
    "print(f\"Total accounts: {len(accounts)}\")\n",
    "print(\"\\nAccount Types:\")\n",
    "print(accounts['account_type'].value_counts())\n",
    "\n",
    "# Select accounts with sufficient data\n",
    "account_summaries = []\n",
    "for _, acc in accounts.iterrows():\n",
    "    summary = db.get_account_summary(acc['account_id'])\n",
    "    if summary['trading_days'] >= 100:\n",
    "        account_summaries.append(summary)\n",
    "\n",
    "print(f\"\\nAccounts with 100+ days: {len(account_summaries)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test feature generation for each account\n",
    "feature_stats = []\n",
    "\n",
    "# Initialize pipeline with all generators\n",
    "pipeline = FeaturePipeline([\n",
    "    TechnicalFeatures(),\n",
    "    BehavioralFeatures(),\n",
    "    MarketRegimeFeatures()\n",
    "])\n",
    "\n",
    "for acc_summary in account_summaries[:5]:  # Test first 5\n",
    "    account_id = acc_summary['account_id']\n",
    "    \n",
    "    # Get data\n",
    "    daily = db.get_account_daily_summary(account_id=account_id)\n",
    "    fills = db.get_fills(account_id=account_id)\n",
    "    \n",
    "    # Generate features\n",
    "    features = pipeline.generate_features(daily, fills, account_id)\n",
    "    \n",
    "    if not features.empty:\n",
    "        stats = {\n",
    "            'account_id': account_id,\n",
    "            'account_type': acc_summary['account_type'],\n",
    "            'n_days': len(daily),\n",
    "            'n_features': len(features.columns) - 1,  # Exclude date\n",
    "            'missing_pct': features.isnull().sum().sum() / features.size * 100,\n",
    "            'tech_features': len([c for c in features.columns if c.startswith('tech_')]),\n",
    "            'behav_features': len([c for c in features.columns if c.startswith('behav_')]),\n",
    "            'regime_features': len([c for c in features.columns if c.startswith('regime_')])\n",
    "        }\n",
    "        feature_stats.append(stats)\n",
    "\n",
    "stats_df = pd.DataFrame(feature_stats)\n",
    "print(\"Feature Generation Statistics:\")\n",
    "stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature quality for best account\n",
    "best_account = account_summaries[0]\n",
    "account_id = best_account['account_id']\n",
    "\n",
    "print(f\"Analyzing account {account_id}\")\n",
    "print(f\"Total P&L: ${best_account['total_pl']:,.2f}\")\n",
    "print(f\"Trading days: {best_account['trading_days']}\")\n",
    "\n",
    "# Get full data\n",
    "daily = db.get_account_daily_summary(account_id=account_id)\n",
    "fills = db.get_fills(account_id=account_id)\n",
    "\n",
    "# Generate features\n",
    "features = pipeline.generate_features(daily, fills, account_id)\n",
    "\n",
    "# Merge with target\n",
    "feature_analysis = features.merge(\n",
    "    daily[['date', 'net']], \n",
    "    on='date', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Create forward target\n",
    "feature_analysis['target'] = feature_analysis['net'].shift(-1)\n",
    "feature_analysis = feature_analysis.dropna(subset=['target'])\n",
    "\n",
    "print(f\"\\nFeature shape: {feature_analysis.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Prepare data\n",
    "feature_cols = [c for c in feature_analysis.columns if c not in ['date', 'net', 'target']]\n",
    "X = feature_analysis[feature_cols].fillna(0)\n",
    "y = feature_analysis['target']\n",
    "\n",
    "# Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, shuffle=False  # Time series split\n",
    ")\n",
    "\n",
    "# Train model\n",
    "rf = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get importance\n",
    "importance = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': rf.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "# Plot top 20 features\n",
    "plt.figure(figsize=(10, 8))\n",
    "top_features = importance.head(20)\n",
    "plt.barh(range(len(top_features)), top_features['importance'])\n",
    "plt.yticks(range(len(top_features)), top_features['feature'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 20 Features by Importance')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Score\n",
    "train_score = rf.score(X_train, y_train)\n",
    "test_score = rf.score(X_test, y_test)\n",
    "print(f\"\\nRandom Forest RÂ² - Train: {train_score:.3f}, Test: {test_score:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_analysis.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze feature categories\n",
    "category_importance = {\n",
    "    'technical': importance[importance['feature'].str.startswith('tech_')]['importance'].sum(),\n",
    "    'behavioral': importance[importance['feature'].str.startswith('behav_')]['importance'].sum(),\n",
    "    'regime': importance[importance['feature'].str.startswith('regime_')]['importance'].sum()\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(category_importance.values(), labels=category_importance.keys(), autopct='%1.1f%%')\n",
    "plt.title('Feature Importance by Category')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation analysis\n",
    "# Select top features\n",
    "top_feature_names = importance.head(15)['feature'].tolist()\n",
    "correlation_data = feature_analysis[top_feature_names + ['target']]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "correlation_matrix = correlation_data.corr()\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', center=0)\n",
    "plt.title('Feature Correlation Matrix')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature stability over time\n",
    "# Calculate rolling statistics for top features\n",
    "stability_window = 60\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(15, 10))\n",
    "\n",
    "for i, feat in enumerate(importance.head(3)['feature']):\n",
    "    # Rolling mean and std\n",
    "    rolling_mean = feature_analysis[feat].rolling(stability_window).mean()\n",
    "    rolling_std = feature_analysis[feat].rolling(stability_window).std()\n",
    "    \n",
    "    axes[i].plot(feature_analysis['date'], feature_analysis[feat], alpha=0.3, label='Value')\n",
    "    axes[i].plot(feature_analysis['date'], rolling_mean, label=f'{stability_window}d Mean')\n",
    "    axes[i].fill_between(\n",
    "        feature_analysis['date'],\n",
    "        rolling_mean - rolling_std,\n",
    "        rolling_mean + rolling_std,\n",
    "        alpha=0.2\n",
    "    )\n",
    "    axes[i].set_title(f'{feat} - Stability Analysis')\n",
    "    axes[i].legend()\n",
    "    axes[i].set_xlabel('Date')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature effectiveness by market regime\n",
    "# Define simple regimes based on volatility\n",
    "volatility = feature_analysis['tech_volatility_20d']\n",
    "vol_percentile = volatility.rank(pct=True)\n",
    "\n",
    "regimes = pd.cut(vol_percentile, bins=[0, 0.33, 0.67, 1.0], labels=['Low Vol', 'Med Vol', 'High Vol'])\n",
    "\n",
    "# Analyze feature correlation by regime\n",
    "regime_correlations = {}\n",
    "for regime in ['Low Vol', 'Med Vol', 'High Vol']:\n",
    "    mask = regimes == regime\n",
    "    if mask.sum() > 20:\n",
    "        regime_data = feature_analysis[mask]\n",
    "        correlations = regime_data[top_feature_names].corrwith(regime_data['target']).abs()\n",
    "        regime_correlations[regime] = correlations\n",
    "\n",
    "# Plot\n",
    "regime_corr_df = pd.DataFrame(regime_correlations)\n",
    "regime_corr_df.plot(kind='bar', figsize=(12, 6))\n",
    "plt.title('Feature Correlation with Target by Volatility Regime')\n",
    "plt.xlabel('Features')\n",
    "plt.ylabel('Absolute Correlation')\n",
    "plt.xticks(rotation=45)\n",
    "plt.legend(title='Regime')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"Feature Engineering Summary:\")\n",
    "print(f\"Total features generated: {len(feature_cols)}\")\n",
    "print(f\"Features with >30% missing: {(feature_analysis[feature_cols].isnull().sum() > len(feature_analysis) * 0.3).sum()}\")\n",
    "print(f\"Constant features: {(feature_analysis[feature_cols].nunique() <= 1).sum()}\")\n",
    "print(f\"\\nTop 10 most important features:\")\n",
    "for i, row in importance.head(10).iterrows():\n",
    "    print(f\"  {row['feature']}: {row['importance']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "risk-tool",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
