{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Trading MVP – Should We Trade Tomorrow?\n",
    "\n",
    "This notebook contains a complete, **reproducible** pipeline:\n",
    "1. Data audit and cleansing\n",
    "2. Feature engineering (rolling behaviour metrics)\n",
    "3. Label creation (predict next‑day bad trading day)\n",
    "4. Expanding‑window time‑series split\n",
    "5. Baseline, logistic‑regression, and LightGBM models\n",
    "6. Evaluation metrics & monetary lift\n",
    "7. Risk‑management dashboard plots\n",
    "\n",
    "Adjust paths and parameters in the first code cell as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "import joblib\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    average_precision_score,\n",
    "    confusion_matrix,\n",
    "    precision_recall_fscore_support,\n",
    ")\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "RANDOM_STATE = 42\n",
    "plt.rcParams[\"figure.figsize\"] = (8, 4)\n",
    "# ---- data path ----\n",
    "CSV_PATH = \"../data/train/predictors_6973.pkl\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "## 1  Load & audit data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_pickle(path)\n",
    "    df = df.sort_index()\n",
    "    return df\n",
    "\n",
    "\n",
    "def audit_data(df: pd.DataFrame):\n",
    "    full_range = pd.date_range(df.index.min(), df.index.max(), freq=\"D\", tz=None)\n",
    "    missing = full_range.difference(df.index)\n",
    "    print(\"Missing days:\", len(missing))\n",
    "    if len(missing):\n",
    "        print(missing[:10])\n",
    "    dupes = df.index[df.index.duplicated()]\n",
    "    print(\"Duplicate days:\", dupes.size)\n",
    "    print(\"\\nNull counts:\\n\", df.isnull().sum())\n",
    "\n",
    "\n",
    "df = load_data(CSV_PATH)\n",
    "audit_data(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.reset_index().columns)\n",
    "print(df.info())\n",
    "# -- print data types\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## 2  Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_streak(series: pd.Series, positive: bool = True) -> pd.Series:\n",
    "    cond = series > 0 if positive else series < 0\n",
    "    streak = np.zeros(len(series), dtype=int)\n",
    "    run = 0\n",
    "    for i, flag in enumerate(cond):\n",
    "        run = run + 1 if flag else 0\n",
    "        streak[i] = run\n",
    "    return pd.Series(streak, index=series.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "windows = [3, 5, 10, 20]\n",
    "# for N in windows:\n",
    "# roll = df[\"total_delta\"].rolling(N, min_periods=1)\n",
    "# df[f\"hit_rate_{N}d\"] = roll.apply(lambda x: (x > 0).mean(), raw=True)\n",
    "# df[f\"avg_win_{N}d\"] = roll.apply(lambda x: x[x > 0].mean() if (x > 0).any() else 0)\n",
    "# df[f\"avg_loss_{N}d\"] = roll.apply(lambda x: x[x < 0].mean() if (x < 0).any() else 0)\n",
    "# df[f\"expectancy_{N}d\"] = (\n",
    "#     df[f\"hit_rate_{N}d\"] * df[f\"avg_win_{N}d\"]\n",
    "#     + (1 - df[f\"hit_rate_{N}d\"]) * df[f\"avg_loss_{N}d\"]\n",
    "# )\n",
    "# df[f\"turnover_qty_{N}d\"] = df[\"qty\"].rolling(N, min_periods=1).sum()\n",
    "# df[f\"sharpe_{N}d\"] = (roll.mean() / (roll.std(ddof=0) + 1e-9)) * np.sqrt(252)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# current streaks (not rolling max)\n",
    "# df[\"win_streak\"] = compute_streak(df[\"total_delta\"], positive=True)\n",
    "# df[\"loss_streak\"] = compute_streak(df[\"total_delta\"], positive=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe(include=\"all\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## 3  Create target label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"pnl_next\"] = df[\"total_delta\"].shift(-1)\n",
    "df[\"tomorrow_bad_day\"] = (df[\"pnl_next\"] < 0).astype(int)\n",
    "df = df.iloc[:-1]  # drop last row (no label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "## 4  Train/Validation/Test split (expanding window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "X = df.drop(columns=[\"tomorrow_bad_day\", \"pnl_next\"])\n",
    "\n",
    "const_cols = [c for c in X.columns if X[c].nunique() <= 1]\n",
    "X = X.drop(columns=const_cols)\n",
    "y = df[\"tomorrow_bad_day\"]\n",
    "splits = list(tscv.split(X))\n",
    "\n",
    "train_idx, test_idx = splits[-1]\n",
    "X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n",
    "X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "## 5  Model training & evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline\n",
    "dummy = DummyClassifier(strategy=\"most_frequent\")\n",
    "dummy.fit(X_train, y_train)\n",
    "# Logistic\n",
    "logreg = LogisticRegression(\n",
    "    penalty=\"l1\",\n",
    "    solver=\"liblinear\",\n",
    "    C=1,\n",
    "    class_weight=\"balanced\",\n",
    "    random_state=RANDOM_STATE,\n",
    ")\n",
    "logreg.fit(X_train, y_train)\n",
    "# LightGBM (quick default)\n",
    "lgbm = lgb.LGBMClassifier(\n",
    "    min_gain_to_split=0,\n",
    "    min_data_in_leaf=5,\n",
    "    random_state=42,\n",
    "    n_estimators=2000,\n",
    "    num_leaves=31,\n",
    "    feature_fraction=0.8,\n",
    "    bagging_fraction=0.8,\n",
    "    learning_rate=0.05,\n",
    "    bagging_freq=1,\n",
    "    metric=\"auc\",\n",
    "    class_weight=\"balanced\",\n",
    ")\n",
    "lgbm.fit(X_train, y_train)\n",
    "\n",
    "models = {\"Dummy\": dummy, \"Logistic\": logreg, \"LightGBM\": lgbm}\n",
    "for name, mdl in models.items():\n",
    "    y_pred = mdl.predict(X_test)\n",
    "    y_prob = mdl.predict_proba(X_test)[:, 1] if hasattr(mdl, \"predict_proba\") else None\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        y_test, y_pred, average=\"binary\", zero_division=0\n",
    "    )\n",
    "    pr_auc = average_precision_score(y_test, y_prob) if y_prob is not None else np.nan\n",
    "    print(\n",
    "        f\"{name}: Precision={prec:.3f} Recall={rec:.3f} F1={f1:.3f} PR_AUC={pr_auc:.3f}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "### Monetary lift calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = lgbm  # choose logistic for demo\n",
    "signals = best_model.predict(X_test)\n",
    "pnl_always = df.loc[y_test.index, \"pnl_next\"].sum()\n",
    "pnl_model = df.loc[y_test.index, \"pnl_next\"].where(signals == 0, 0).sum()\n",
    "print(\"Always trade PnL:\", pnl_always)\n",
    "print(\"Model guided PnL:\", pnl_model)\n",
    "print(\"Lift:\", pnl_model - pnl_always)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "## 6  Risk‑management dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equity_curve(pnl_series):\n",
    "    return pnl_series.cumsum()\n",
    "\n",
    "\n",
    "pnl_series = df.loc[y_test.index, \"pnl_next\"]\n",
    "pnl_model_series = df.loc[y_test.index, \"pnl_next\"].where(signals == 0, 0)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(equity_curve(pnl_series), label=\"Always\")\n",
    "plt.plot(equity_curve(pnl_model_series), label=\"Model\")\n",
    "plt.title(\"Cumulative Equity\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def equity_curve(p):\n",
    "    return p.cumsum()\n",
    "\n",
    "\n",
    "jj = 0\n",
    "hh = -1\n",
    "\n",
    "eq_always = equity_curve(pnl_series)[jj:hh]\n",
    "eq_model = equity_curve(pnl_model_series)[jj:hh]\n",
    "\n",
    "bad_mask = signals[jj:hh] == 1  # model skipped\n",
    "bad_idx = y_test[jj:hh].index[bad_mask]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "ax.plot(eq_always, label=\"Always\")\n",
    "ax.plot(eq_model, label=\"Model\")\n",
    "\n",
    "# mark skip days on always-trade curvea\n",
    "ax.scatter(\n",
    "    bad_idx, eq_always.loc[bad_idx], marker=\"v\", color=\"red\", s=40, label=\"Skipped\"\n",
    ")\n",
    "\n",
    "ax.set_title(\"Cumulative Equity with Skip Markers\")\n",
    "ax.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "booster = lgbm.booster_  # unwrap sklearn wrapper\n",
    "\n",
    "gain = booster.feature_importance(importance_type=\"gain\")\n",
    "names = booster.feature_name()\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "imp = pd.DataFrame({\"feature\": names, \"gain\": gain}).sort_values(\n",
    "    \"gain\", ascending=False\n",
    ")\n",
    "imp.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "gain = booster.feature_importance(importance_type=\"gain\")\n",
    "names = booster.feature_name()\n",
    "imp = {n: g for n, g in zip(names, gain)}\n",
    "\n",
    "total = sum(gain)\n",
    "pct = {n: g / total for n, g in imp.items()}\n",
    "\n",
    "display(pct)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ds-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
